head -100000 europarl.txt > europarl_100k.txt





19 fevr:


environement openNMP
gensim
numpy


2e sujet : faire git clone

8 Go en tout (sans dataset, entrainement du modèle)



pour exo 2 :
2) utiliser corpus en forme fléchie (tels qu'on les récupère sur le site, directement sur OPUS (version tokenisée du corpus récupéré de la base OPUS))

3) corpus en lemmes (substantif au singulier, verbe à l'infinitif...)
 pour lemmatiser    NLTK :  EN: WordNet Lemmatizer
                            FR: French Leff Lemmatizer

                            => Faire un POS tagging avant la lemmatisation (dans le NLTK)
                            sinon :
                            président =>    NOM: président 
                                            VERBE: présider
                            sinon NLTK a tout le reste
                    
                    ou
                    spacey : lemmatise directement
 pour lemmatiser vous pouvez utiliser Spacy (c'est pas mal)

pour les plus motivés : faire les deux (tout le M dit que Spacy est meilleur, peut-on le prouver?)
comparaison : faire juste un diff entre les deux





1er étape : installer




sujet 1:
1) code source
2) installer
3) expérimenter
4) évaluer


sujet 2:
1) code
2) installer (utiliser que les outils des LLMs)
3) Expérimenter un composant existant LLMs (c une boite noire contrairement à un open source)
4) évaluer

pour le prof : voir si les LLM c'est meilleur





à rendre :
.yaml
le code
README
rapport pdf

ignorer .pt













utiliser Google Collab de préférence
lancer une session de Google Collab, la laisser tourner

Google Colab : version de OpenNMT: Trnasformer

Code source (git) : version de OpenNMT: LSTM








Experimentation 1                               Expérimentation 2
Tokenisation, Maj->Min, Taille de phrases       Pas de tokenisation
                        (80 caractères)         Pas de Maj->Min
TRAIN                                           Taille de phases (80 caractères)
DEV                                             TRAIN
TEST                                            DEV
                                                TEST



dans Moses, y a un script : multi_bleu.pl
Score: BLEU (Mesurer la qualité de traduction)
>0.5 (Bonne traduction)




Tokenisation:
- WordPiece (BERT)
- BPE (Byle Pair Encoding)
Cette loi est anticonstitutionnelle

1) WordPiece ()
Cet # te loi est ant # constitution # nel # le.
2)Moses (multi_bleu.pl)
Cette loi est anti constitutionnelle
l'_art

faire la même tokenisation pour les tests

score bleue envoyée par le prof

faire tout avec Moses





peut aussi utiliser kagel (si google colab est trop limité)






il préfère un lien Github
mail
en bas : 2 personnes qui y travaillent

pièce joint : code source, rapport en pdf, data (10k, 100k phrases)

ne pas hésiter à faire des remarques, proposer des idées